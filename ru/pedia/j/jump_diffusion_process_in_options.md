# Процессы принятия решений Маркова (MDP)

## Введение
Процессы принятия решений Маркова (MDP) - это математические структуры, используемые для моделирования принятия решений в средах, где результаты частично случайны и частично под контролем лица, принимающего решения. MDP широко используются в различных областях, включая робототехнику, экономику, производство и финансы. Они обеспечивают строгую основу для анализа и оптимизации проблем последовательного принятия решений, где каждое действие влияет не только на немедленные вознаграждения, но и на будущие состояния и вознаграждения.

## Компоненты MDP
MDP обычно определяется следующими компонентами:

### Состояния (\(S\))
Множество всех возможных состояний \(S\) представляет каждую мыслимую ситуацию, в которой может оказаться агент. Каждое состояние содержит всю информацию, необходимую для принятия решения.
- **Пример**: В торговой среде акций состояние может быть представлено текущими ценами различных акций, доступными средствами и существующими холдингами портфеля.

### Действия (\(A\))
Множество всех возможных действий \(A\) включает все решения, которые агент может принять в любом состоянии.
- **Пример**: В той же торговой среде акций действия могут включать покупку, продажу или удержание акции.

### Вероятности перехода (\(T\) или \(P\))
Вероятности перехода \( P(s'|s,a) \) описывают вероятность перехода из состояния \( s \) в состояние \( s' \) при действии \( a \). Вероятности должны удовлетворять свойству Маркова, подразумевая, что будущее состояние зависит только от текущего состояния и действия, а не от истории состояний и действий.
- **Пример**: Если агент покупает акцию, вероятность перехода оценит вероятность различных изменений в цене акции.

### Вознаграждения (\(R\))
Функция вознаграждения \( R(s,a,s') \) определяет немедленное вознаграждение, полученное после перехода из состояния \( s \) в состояние \( s' \) из-за действия \( a \). Это количественно представляет выгоду от выполнения конкретного действия в данном состоянии.
- **Пример**: В торговой среде вознаграждением может быть прибыль или убыток в результате покупки или продажи акции.

## Формальное определение
MDP можно формально определить как кортеж \( (S, A, P, R, \gamma) \), где:
- \( S \) - это пространство состояний
- \( A \) - это пространство действий
- \( P \) - это функция вероятности перехода
- \( R \) - это функция вознаграждения
- \( \gamma \) - это коэффициент дисконтирования, который определяет текущую стоимость будущих вознаграждений

## Политики
Политика \( \pi \) - это стратегия, используемая агентом, определяющая действие для выполнения в каждом состоянии. Политики могут быть детерминированными (\( \pi(s) = a \)) или стохастическими (\( \pi(a|s) \)), где последнее является распределением вероятностей над действиями при данном состоянии.

### Оптимальная политика
Цель в MDP - найти оптимальную политику \( \pi^* \), которая максимизирует ожидаемое кумулятивное вознаграждение с течением времени. Это часто формализуется с использованием функции стоимости или функции стоимости действия.

## Функции стоимости
Функции стоимости используются для оценки долгосрочных выгод состояний или пар состояние-действие при данной политике. Два основных типа функций стоимости:

### Функция стоимости состояния (\( V^\pi(s) \))
Функция стоимости состояния \( V^\pi(s) \) представляет ожидаемое кумулятивное вознаграждение от начала в состоянии \( s \) и следования политике \( \pi \).

\[ V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t+1}) \mid s_0 = s \right] \]

### Функция стоимости действия (\( Q^\pi(s, a) \))
Функция стоимости действия \( Q^\pi(s, a) \) представляет ожидаемое кумулятивное вознаграждение от начала в состоянии \( s \), выполнения действия \( a \) и последующего следования политике \( \pi \).

\[ Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t+1}) \mid s_0 = s, a_0 = a \right] \]

### Уравнения Беллмана
Уравнения Беллмана обеспечивают рекурсивные отношения для функций стоимости, необходимые для решения MDP.

#### Уравнение ожидания Беллмана для \( V^\pi \)

\[ V^\pi(s) = \sum_{a \in A} \pi(a|s) \left[ R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) V^\pi(s') \right] \]

#### Уравнение ожидания Беллмана для \( Q^\pi \)

\[ Q^\pi(s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) \sum_{a' \in A} \pi(a'|s') Q^\pi(s', a') \]

## Методы решения
Существует несколько методов для нахождения оптимальных политик в MDP, классифицированных на динамическое программирование, методы Монте-Карло и методы временной разности.

### Динамическое программирование
Методы динамического программирования используют уравнения Беллмана для итеративного улучшения оценок стоимости и политик. Ключевые алгоритмы включают:

#### Итерация по стоимости
Итерация по стоимости многократно обновляет функцию стоимости состояния до тех пор, пока она не сойдется к оптимальной функции стоимости.

\[ V_{k+1}(s) = \max_{a \in A} \left[ R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) V_k(s') \right] \]

#### Итерация по политике
Итерация по политике чередуется между оценкой политики (оценкой функции стоимости для данной политики) и улучшением политики (генерацией лучшей политики на основе текущей функции стоимости).

1. **Оценка политики**: Решение \( V^\pi(s) \) для текущей политики \( \pi \).
2. **Улучшение политики**: Обновление политики \( \pi \) с использованием обновленной функции стоимости.

\[ \pi'(s) = \arg\max_{a \in A} \left[ R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) V^\pi(s') \right] \]

### Методы Монте-Карло
Методы Монте-Карло оценивают функции стоимости на основе выборочных эпизодов из MDP. Эти методы не требуют знания вероятностей перехода и используются в контекстах без модели.

#### Первого посещения MC
Монте-Карло первого посещения обновляет функцию стоимости на основе первого вхождения каждого состояния или пары состояние-действие в эпизоде.

#### Каждого посещения MC
Монте-Карло каждого посещения обновляет функцию стоимости на основе каждого вхождения каждого состояния или пары состояние-действие в эпизоде.

### Обучение с временной разностью (TD)
Методы временной разности объединяют идеи динамического программирования и методов Монте-Карло. TD обучение обновляет оценки стоимости на основе разности (временной разности) между последовательными оценками стоимости.

#### TD(0)
TD(0) - это простейшая форма, обновляющая функцию стоимости с использованием одношагового прогноза.

\[ V(s_t) \leftarrow V(s_t) + \alpha \left[ R_{t+1} + \gamma V(s_{t+1}) - V(s_t) \right] \]

#### Q-обучение
Q-обучение - это алгоритм управления TD вне политики, который изучает оптимальную политику независимо от действий агента.

\[ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ R_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right] \]

#### SARSA
SARSA - это алгоритм управления TD на политике, который обновляет функцию стоимости действия на основе действий агента, следующих текущей политике.

\[ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right] \]

## Применения в алгоритмической торговле
MDP имеют значительные применения в алгоритмической торговле, где агенты должны принимать последовательные решения в условиях неопределенности для максимизации доходности.

### Оптимальное исполнение
Одно из ключевых применений - оптимальное исполнение сделки, при котором агент определяет лучшие времена и количества для торговли, чтобы минимизировать транзакционные издержки и рыночное воздействие.

### Управление портфелем
MDP могут использоваться для динамической оптимизации портфеля, где агент корректирует свои холдинги в ответ на изменяющиеся рыночные условия для достижения баланса между риском и доходностью.

### Маркет-мейкинг
Агенты могут использовать MDP для разработки оптимальных стратегий маркет-мейкинга, устанавливая спреды bid-ask на основе вероятных будущих движений цен для максимизации прибыли от спреда при управлении риском инвентаря.

### Обучение с подкреплением в торговле
Обучение с подкреплением, охватывающее многие методы решения MDP, используется для разработки сложных торговых алгоритмов. Платформы, такие как Alpaca и QuantConnect, предоставляют среды для тестирования и развертывания этих стратегий.

## Заключение
Процессы принятия решений Маркова предлагают надежную структуру для решения проблем последовательного принятия решений в стохастических средах. Благодаря своей способности балансировать немедленные и будущие вознаграждения, они незаменимы в различных областях, особенно в алгоритмической торговле, где они помогают разрабатывать стратегии для оптимизации финансовой доходности в условиях неопределенности. Через методы, включая динамическое программирование, Монте-Карло и обучение с временной разностью, MDP облегчают разработку оптимальных политик для сложных реальных проблем.
